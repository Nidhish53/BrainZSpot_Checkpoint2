{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alphabet Recognition System using Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Network (CNN) is a Deep Learning Algorithm widely used for character recognition. This algorithm identifies the alphabet from the given input image.\n",
    "\n",
    "The accuracy achieved using this algorithm is 93.42%.\n",
    "\n",
    "## 1. Anvil Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to wss://anvil.works/uplink\n",
      "Anvil websocket open\n",
      "Authenticated OK\n"
     ]
    }
   ],
   "source": [
    "import anvil.server\n",
    "import anvil.media\n",
    "anvil.server.connect(\"44STZQZQTYAAVHHWGRPMXDRN-P5VUQGPD3HW5UMWJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyttsx3\n",
      "  Downloading pyttsx3-2.90-py3-none-any.whl (39 kB)\n",
      "Collecting pypiwin32\n",
      "  Downloading pypiwin32-223-py3-none-any.whl (1.7 kB)\n",
      "Requirement already satisfied: comtypes in c:\\users\\shree\\anaconda3\\lib\\site-packages (from pyttsx3) (1.1.10)\n",
      "Requirement already satisfied: pywin32 in c:\\users\\shree\\anaconda3\\lib\\site-packages (from pyttsx3) (302)\n",
      "Installing collected packages: pypiwin32, pyttsx3\n",
      "Successfully installed pypiwin32-223 pyttsx3-2.90\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "#randomLetter = 'h'\n",
    "randomLetter = random.choice(string.ascii_letters)\n",
    "modified_string = randomLetter.translate({ord('h'): None, ord('q'): None})\n",
    "#print(randomLetter)\n",
    "\n",
    "import pyttsx3\n",
    "text_speech=pyttsx3.init()\n",
    "#ans=input()\n",
    "text_speech.say(\"Please write the letter\")\n",
    "text_speech.say(randomLetter)\n",
    "text_speech.runAndWait()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 15, 15, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 13, 13, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 6, 6, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               147584    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 26)                3354      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 161,082\n",
      "Trainable params: 161,082\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape = (32,32,3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 128, activation = 'relu'))\n",
    "model.add(Dense(units = 26, activation = 'softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 501 images belonging to 26 classes.\n",
      "Found 261 images belonging to 26 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory = 'Training',\n",
    "    target_size = (32,32),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical'\n",
    "\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory = 'Testing',\n",
    "    target_size = (32,32),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical'\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shree\\AppData\\Local\\Temp\\ipykernel_11576\\2069286868.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(train_generator,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - ETA: 0s - loss: 3.1301 - accuracy: 0.1457WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 16 batches). You may need to use the repeat() function when building your dataset.\n",
      "16/16 [==============================] - 3s 112ms/step - loss: 3.1301 - accuracy: 0.1457 - val_loss: 2.8933 - val_accuracy: 0.1839\n",
      "Epoch 2/3\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 2.1835 - accuracy: 0.4571\n",
      "Epoch 3/3\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 1.0424 - accuracy: 0.6866\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                         steps_per_epoch = 16,\n",
    "                         epochs = 3,\n",
    "                         validation_data = test_generator,\n",
    "                         validation_steps = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Saving/Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\\conv2d\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\conv2d_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\flatten\n",
      "......vars\n",
      "...layers\\max_pooling2d\n",
      "......vars\n",
      "...layers\\max_pooling2d_1\n",
      "......vars\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...metrics\\mean_metric_wrapper\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........11\n",
      ".........12\n",
      ".........13\n",
      ".........14\n",
      ".........15\n",
      ".........16\n",
      ".........2\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-03-11 18:52:55         3090\n",
      "metadata.json                                  2023-03-11 18:52:55           64\n",
      "variables.h5                                   2023-03-11 18:52:55      1965088\n"
     ]
    }
   ],
   "source": [
    "pickle.dump(model, open('CNN_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-03-11 18:52:54         3090\n",
      "metadata.json                                  2023-03-11 18:52:54           64\n",
      "variables.h5                                   2023-03-11 18:52:54      1965088\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\\conv2d\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\conv2d_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\flatten\n",
      "......vars\n",
      "...layers\\max_pooling2d\n",
      "......vars\n",
      "...layers\\max_pooling2d_1\n",
      "......vars\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...metrics\\mean_metric_wrapper\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........11\n",
      ".........12\n",
      ".........13\n",
      ".........14\n",
      ".........15\n",
      ".........16\n",
      ".........2\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n"
     ]
    }
   ],
   "source": [
    "model = pickle.load(open('CNN_model.sav','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(result):\n",
    "    if result[0][0] == 1:\n",
    "        return('a')\n",
    "    elif result[0][1] == 1:\n",
    "        return ('b')\n",
    "    elif result[0][2] == 1:\n",
    "        return ('c')\n",
    "    elif result[0][3] == 1:\n",
    "        return ('d')\n",
    "    elif result[0][4] == 1:\n",
    "        return ('e')\n",
    "    elif result[0][5] == 1:\n",
    "        return ('f')\n",
    "    elif result[0][6] == 1:\n",
    "        return ('g')\n",
    "    elif result[0][7] == 1:\n",
    "        return ('h')\n",
    "    elif result[0][8] == 1:\n",
    "        return ('i')\n",
    "    elif result[0][9] == 1:\n",
    "        return ('j')\n",
    "    elif result[0][10] == 1:\n",
    "        return ('k')\n",
    "    elif result[0][11] == 1:\n",
    "        return ('l')\n",
    "    elif result[0][12] == 1:\n",
    "        return ('m')\n",
    "    elif result[0][13] == 1:\n",
    "        return ('n')\n",
    "    elif result[0][14] == 1:\n",
    "        return ('o')\n",
    "    elif result[0][15] == 1:\n",
    "        return ('p')\n",
    "    elif result[0][16] == 1:\n",
    "        return ('q')\n",
    "    elif result[0][17] == 1:\n",
    "        return ('r')\n",
    "    elif result[0][18] == 1:\n",
    "        return ('s')\n",
    "    elif result[0][19] == 1:\n",
    "        return ('t')\n",
    "    elif result[0][20] == 1:\n",
    "        return ('u')\n",
    "    elif result[0][21] == 1:\n",
    "        return ('v')\n",
    "    elif result[0][22] == 1:\n",
    "        return ('w')\n",
    "    elif result[0][23] == 1:\n",
    "        return ('x')\n",
    "    elif result[0][24] == 1:\n",
    "        return ('y')\n",
    "    elif result[0][25] == 1:\n",
    "        return ('z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted Alphabet is: l\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename = r'Testing/'+randomLetter+'/24.png'\n",
    "test_image = tf.keras.utils.load_img(filename, target_size = (32,32))\n",
    "plt.imshow(test_image)\n",
    "test_image =  tf.keras.utils.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "\n",
    "result = model.predict(test_image)\n",
    "result = get_result(result)\n",
    "print ('Predicted Alphabet is: {}'.format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Predicting the Alphabet from the Input Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of code is receives the input image from the anvil website and returns the predicted alphabet back to the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def model_run(path):\n",
    "    with anvil.media.TempFile(path) as filename:\n",
    "        test_image = image.load_img(filename, target_size = (32,32))\n",
    "        test_image = image.img_to_array(test_image)\n",
    "        test_image = np.expand_dims(test_image, axis = 0)\n",
    "        result = model.predict(test_image)\n",
    "        result = get_result(result)\n",
    "        return ('Predicted Alphabet is: {}'.format(result))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
